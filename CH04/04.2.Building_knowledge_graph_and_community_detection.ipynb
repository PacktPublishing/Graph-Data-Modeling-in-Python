{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buidling the Knowledge Graph and performing Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to install workbook requirements\n",
    "#!pip install -r requirements.txt\n",
    "#!pip install en_core_sci_sm-0.5.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph\n",
    "import csv\n",
    "import scispacy\n",
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing a knowledge graph schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./data/20k_abstracts_clean.csv', 'r') as c:\n",
    "\treader = csv.reader(c)\n",
    "\tdata = [line for line in reader]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_sci_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data[0][1]\n",
    "doc = nlp(text)\n",
    "print(list(doc.ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_entities = [[row[0], nlp(row[1]).ents] for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_entities = [[row[0], [str(ent).lower() for ent in row[1]]] for row in abstract_entities]\n",
    "print(abstract_entities[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities = [row[1] for row in abstract_entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "entities = itertools.chain.from_iterable(all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "entity_freq = dict(Counter(entities))\n",
    "entity_freq = dict(sorted(entity_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "print(entity_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_freq = {ent: value for ent, value in entity_freq.items() if value > 100}\n",
    "print(len(high_freq))\n",
    "print(len(entity_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_freq = {ent: value for ent, value in entity_freq.items() if value == 1}\n",
    "print(len(low_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_terms = [ent for ent, value in entity_freq.items() if value > 100 or value == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_entities = [[row[0], [ent for ent in row[1] if ent not in removed_terms]]\n",
    "                  \tfor row in abstract_entities]\n",
    "print(abstract_entities[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [abstract[1] for abstract in abstract_entities]  # get just ents no abstract IDs\n",
    "unique_terms = list(set(itertools.chain.from_iterable(terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_ids = {term: i for i, term in enumerate(unique_terms, len(data))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(term_ids['ige'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist = []\n",
    "for abstract_id, terms in abstract_entities:\n",
    "    term_freq = dict(Counter(terms))\n",
    "    for term, freq in term_freq.items():\n",
    "        edgelist.append([int(term_ids[term]), int(abstract_id), freq])\n",
    " \n",
    "print(edgelist[:10])\n",
    "assert [term_ids['ige'], 0, 4] in edgelist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = igraph.Graph(directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add_vertices(len(term_ids) + len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [abstract[1] for abstract in data] + [ent for ent, _ in term_ids.items()]\n",
    "g.vs['text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.vs[term_ids['ige']]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['abstract' for _ in data] + ['term' for _ in term_ids.items()]\n",
    "g.vs['type'] = types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.vs[term_ids['ige']]['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = [[source, target] for source, target, _ in edgelist]\n",
    "frequencies = [freq for _, _, freq in edgelist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add_edges(edges)\n",
    "g.es['frequency'] = frequencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge graph analysis and community detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examing knowledge graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(g.vs))\n",
    "print(len(g.es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_components = g.clusters(mode='weak')\n",
    "print(connected_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_nodes = g.vs.select(type_eq='abstract')\n",
    "term_nodes = g.vs.select(type_eq='term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_degree = g.degree(abstract_nodes)\n",
    "term_degree = g.degree(term_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(abstract_degree, bins=20, edgecolor='black')\n",
    "plt.xlabel('Abstract Node Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('charts/abstract_degree.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(term_degree, bins=20, edgecolor='black')\n",
    "plt.xlabel('Term Node Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('charts/term_node_degree.jpg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying abstracts of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoga_node_id = g.vs.select(text_eq='yoga')[0].index\n",
    "yoga_abstract_nodes = g.neighbors(g.vs[yoga_node_id])\n",
    "yoga_abstracts = [g.vs[neighbor]['text'] for neighbor in yoga_abstract_nodes]\n",
    "print(yoga_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_term_nodes = [g.neighbors(node) for node in yoga_abstract_nodes]\n",
    "import itertools\n",
    "related_term_nodes = set(itertools.chain.from_iterable(related_term_nodes))\n",
    "related_terms = g.vs(related_term_nodes)['text']\n",
    "print(related_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(related_terms))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying fields with Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_u = g.as_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_membership = g_u.community_multilevel()\n",
    "print(len(community_membership))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, community in enumerate(community_membership):\n",
    "\tsize = len(community)\n",
    "\tprint(f'Community: {i}, size: {size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallest_community = sorted(list(community_membership), key=len)[0]\n",
    "print(smallest_community)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_nodes = g.vs[smallest_community].select(type_eq='term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_terms = community_nodes['text']\n",
    "print(community_terms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing build_know_graph.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile build_know_graph.py\n",
    "\"\"\" \n",
    "Name:       build_know_graph.py.py\n",
    "Author(s):  Gary Hutson & Matt Jackson on behalf of Packt publishing\n",
    "Date:       03/02/2022\n",
    "Usage:      build_know_graph.py\n",
    "\"\"\"\n",
    "import igraph\n",
    "import csv\n",
    "import scispacy\n",
    "import spacy\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load in data\n",
    "with open('./data/20k_abstracts_clean.csv', 'r') as c:\n",
    "\treader = csv.reader(c)\n",
    "\tdata = [line for line in reader]\n",
    "\n",
    "# Load in spacy NLP library\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "text = data[0][1]\n",
    "doc = nlp(text)\n",
    "print(list(doc.ents))\n",
    "\n",
    "# Get abstract entities \n",
    "abstract_entities = [[row[0], nlp(row[1]).ents] for row in data]\n",
    "abstract_entities = [[row[0], [str(ent).lower() for ent in row[1]]] for row in abstract_entities]\n",
    "print(abstract_entities[:5])\n",
    "\n",
    "# Get all entities\n",
    "all_entities = [row[1] for row in abstract_entities]\n",
    "\n",
    "# Use itertools to get all entities\n",
    "entities = itertools.chain.from_iterable(all_entities)\n",
    "\n",
    "# Get frequency counts using Counter()\n",
    "entity_freq = dict(Counter(entities))\n",
    "entity_freq = dict(sorted(entity_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "print(entity_freq)\n",
    "\n",
    "high_freq = {ent: value for ent, value in entity_freq.items() if value > 100}\n",
    "print(len(high_freq))\n",
    "print(len(entity_freq))\n",
    "\n",
    "low_freq = {ent: value for ent, value in entity_freq.items() if value == 1}\n",
    "print(len(low_freq))\n",
    "\n",
    "# Remove some terms\n",
    "removed_terms = [ent for ent, value in entity_freq.items() if value > 100 or value == 1]\n",
    "abstract_entities = [[row[0], [ent for ent in row[1] if ent not in removed_terms]]\n",
    "                  \tfor row in abstract_entities]\n",
    "print(abstract_entities[0])\n",
    "\n",
    "# Constructing the knowledge graph\n",
    "terms = [abstract[1] for abstract in abstract_entities]  # get just ents no abstract IDs\n",
    "unique_terms = list(set(itertools.chain.from_iterable(terms)))\n",
    "\n",
    "\n",
    "term_ids = {term: i for i, term in enumerate(unique_terms, len(data))}\n",
    "print(term_ids['ige'])\n",
    "\n",
    "# Create the edgelists\n",
    "edgelist = []\n",
    "for abstract_id, terms in abstract_entities:\n",
    "    term_freq = dict(Counter(terms))\n",
    "    for term, freq in term_freq.items():\n",
    "        edgelist.append([int(term_ids[term]), int(abstract_id), freq])\n",
    " \n",
    "print(edgelist[:10])\n",
    "assert [term_ids['ige'], 0, 4] in edgelist\n",
    "\n",
    "# Instantiate igraph as a directed graph\n",
    "g = igraph.Graph(directed=True)\n",
    "# Add verticies\n",
    "g.add_vertices(len(term_ids) + len(data))\n",
    "# Extract text\n",
    "text = [abstract[1] for abstract in data] + [ent for ent, _ in term_ids.items()]\n",
    "g.vs['text'] = text\n",
    "print(g.vs[term_ids['ige']]['text'])\n",
    "types = ['abstract' for _ in data] + ['term' for _ in term_ids.items()]\n",
    "g.vs['type'] = types\n",
    "print(g.vs[term_ids['ige']]['type'])\n",
    "edges = [[source, target] for source, target, _ in edgelist]\n",
    "frequencies = [freq for _, _, freq in edgelist]\n",
    "g.add_edges(edges)\n",
    "g.es['frequency'] = frequencies\n",
    "# Analyze and apply community detection\n",
    "print(len(g.vs))\n",
    "print(len(g.es))\n",
    "# Find weakly connected components\n",
    "connected_components = g.clusters(mode='weak')\n",
    "print(connected_components)\n",
    "# Get the abstract and term nodes\n",
    "abstract_nodes = g.vs.select(type_eq='abstract')\n",
    "term_nodes = g.vs.select(type_eq='term')\n",
    "# Get the degrees\n",
    "abstract_degree = g.degree(abstract_nodes)\n",
    "term_degree = g.degree(term_nodes)\n",
    "\n",
    "# Do some plotting\n",
    "# Abstract degree first\n",
    "plt.hist(abstract_degree, bins=20, edgecolor='black')\n",
    "plt.xlabel('Abstract Node Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('charts/abstract_degree.jpg')\n",
    "plt.show()\n",
    "\n",
    "# Term degree second\n",
    "plt.hist(term_degree, bins=20, edgecolor='black')\n",
    "plt.xlabel('Term Node Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('charts/term_node_degree.jpg')\n",
    "\n",
    "# Identify abstracts of interest\n",
    "yoga_node_id = g.vs.select(text_eq='yoga')[0].index\n",
    "yoga_abstract_nodes = g.neighbors(g.vs[yoga_node_id])\n",
    "yoga_abstracts = [g.vs[neighbor]['text'] for neighbor in yoga_abstract_nodes]\n",
    "print(yoga_abstracts)\n",
    "\n",
    "related_term_nodes = [g.neighbors(node) for node in yoga_abstract_nodes]\n",
    "import itertools\n",
    "related_term_nodes = set(itertools.chain.from_iterable(related_term_nodes))\n",
    "related_terms = g.vs(related_term_nodes)['text']\n",
    "print(related_terms)\n",
    "print(len(related_terms))\n",
    "\n",
    "# Identifying fields with Community Detection\n",
    "g_u = g.as_undirected()\n",
    "community_membership = g_u.community_multilevel()\n",
    "print(len(community_membership))\n",
    "\n",
    "# Loop through communities and return their sizings\n",
    "for i, community in enumerate(community_membership):\n",
    "\tsize = len(community)\n",
    "\tprint(f'Community: {i}, size: {size}')\n",
    "\n",
    "# Get the smallest community\n",
    "smallest_community = sorted(list(community_membership), key=len)[0]\n",
    "print(smallest_community)\n",
    "\n",
    "# Get the community nodes\n",
    "community_nodes = g.vs[smallest_community].select(type_eq='term')\n",
    "community_terms = community_nodes['text']\n",
    "print(community_terms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e962125a6bade716827c7cd07d3b7c9b717838910a5075eb32d8b321480d4bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
